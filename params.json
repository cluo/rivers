{"name":"Rivers","tagline":"Data Stream Processing API for GO","body":"# Rivers ![Basic Stream](https://raw.githubusercontent.com/drborges/rivers/master/https://raw.githubusercontent.com/drborges/rivers/master/docs/rivers-logo.png)\r\n\r\n[![Build Status](https://travis-ci.org/drborges/rivers.svg?branch=master)](https://travis-ci.org/drborges/rivers)\r\n\r\nData Stream Processing API for GO\r\n\r\n# Overview\r\n\r\nRivers provides a simple though powerful API for processing streams of data built on top of `goroutines`, `channels` and the [pipeline pattern](https://blog.golang.org/pipelines).\r\n\r\n```go\r\nerr := rivers.From(NewGithubRepositoryProducer(httpClient)).\r\n\tFilter(hasForks).\r\n\tFilter(hasRecentActivity).\r\n\tDrop(ifStarsCountIsLessThan(50)).\r\n\tMap(extractAuthorInformation).\r\n\tBatch(200).\r\n\tEach(saveBatch).\r\n\tDrain()\r\n```\r\n\r\nWith a few basic building blocks based on the `Producer-Consumer` model, you can compose and create complex data processing pipelines for solving a variety of problems.\r\n\r\nA pipeline often assumes the following format: `Producer`, one or more `Transformers` and an optional `Consumer`\r\n\r\n![Basic Stream](https://raw.githubusercontent.com/drborges/rivers/master/docs/stream-transformation.png)\r\n\r\nFor more complex formats check the `Combiners` and `Dispatchers` sections.\r\n\r\n# Building Blocks\r\n\r\nA particular stream pipeline may be built composing building blocks such as `producers`,  `consumers`,  `transformers`, `combiners` and `dispatchers`.\r\n\r\n### Stream ![Basic Stream](https://raw.githubusercontent.com/drborges/rivers/master/docs/stream.png)\r\n\r\nStreams are simply readable or writable channels where data flows through `asynchronously`. They are usually created by `producers` providing data from a particular data source, for example `files`, `network` (socket data, API responses), or even as simple as regular `slice` of data to be processed.\r\n\r\nRivers provides a `stream` package with a constructor function for creating streams as follows:\r\n\r\n```go\r\ncapacity := 100\r\nreadable, writable := stream.New(capacity)\r\n```\r\n\r\nStreams are buffered and the `capacity` parameter dictates how many items can be produced into the stream without being consumed until the producer is blocked. This blocking mechanism is natively implemented by Go channels, and is a form of `back-pressuring` the pipeline.\r\n\r\n### Producers ![Basic Stream](https://raw.githubusercontent.com/drborges/rivers/master/docs/producer.png)\r\n\r\nAsynchronously emits data into a stream. Any type implementing the `stream.Producer` interface can be used as a producer in rivers.\r\n\r\n```go\r\ntype Producer interface {\r\n\tProduce() stream.Readable\r\n}\r\n```\r\n\r\nProducers implement the [pipeline pattern](https://blog.golang.org/pipelines) in order to asynchronously produce items that will be eventually consumed by a further stage in the pipeline. Rivers provides a few implementations of producers such as:\r\n\r\n- `rivers.FromRange(0, 1000)`\r\n- `rivers.FromSlice(slice)`\r\n- `rivers.FromData(1, 2, \"a\", \"b\", Person{Name:\"Diego\"})`\r\n- `rivers.FromFile(aFile).ByLine()`\r\n- `rivers.FromSocket(\"tcp\", \":8484\")`\r\n\r\nA good producer implementation takes care of at least 3 important aspects:\r\n\r\n1. Checks if rivers context is still opened before emitting any item\r\n2. Defers the recover function from rivers context as part of the goroutine execution (For more see `Cancellation` topic)\r\n3. Closes the writable stream at the end of the go routine. By closing the channel further stages of the pipeline know when their work is done.\r\n\r\nLets see how one would go about converting a slice of numbers into a stream with a simple Producer implementation:\r\n\r\n```go\r\ntype NumbersProducer struct {\r\n\tcontext stream.Context\r\n\tnumbers []int\r\n}\r\n\r\nfunc (producer *NumbersProducer) Produce() stream.Readable {\r\n\treadable, writable := stream.New(len(producer.numbers))\r\n\r\n\tgo func() {\r\n\t\tdefer producer.context.Recover()\r\n\t\tdefer close(writable)\r\n\r\n\t\tfor _, n := range producer.numbers {\r\n\t\t\tselect {\r\n\t\t\tcase <-producer.context.Closed():\r\n\t\t\t\treturn\r\n\t\t\tdefault:\r\n\t\t\t\twritable <- n\r\n\t\t\t}\r\n\t\t}\r\n\t}()\r\n\r\n\treturn readable\r\n}\r\n```\r\n\r\nThe code above is a complaint `rivers.Producer` implementation and it gives the developer full control of the process. Rivers also provides an `Observable` type that implements `stream.Producer` covering the basic 3 aspects mentioned above that you can use for most cases: `producers.Observable`.\r\n\r\nOur producer implementation in terms of an observable would then look like:\r\n\r\n```go\r\nfunc NewNumbersProducer(numbers []int) stream.Producer {\r\n\treturn &Observable{\r\n\t\tCapacity: len(numbers),\r\n\t\tEmit: func(emitter stream.Emitter) {\r\n\t\t\tfor _, n := range numbers {\r\n\t\t\t\temitter.Emit(n)\r\n\t\t\t}\r\n\t\t},\r\n\t}\r\n}\r\n```\r\n\r\n### Consumers ![Basic Stream](https://raw.githubusercontent.com/drborges/rivers/master/docs/consumer.png)\r\n\r\nConsumes data from a particular stream. Consumers block the process until there is no more data to be consumed out of the stream.\r\n\r\nYou can use consumers to collect the items reaching the end of the pipeline, or any errors that might have happened during the execution.\r\n\r\nIt is very likely you will most often need a final consumer in your pipeline for waiting for the pipeline result before moving on.\r\n\r\nRivers has a few built-in consumers, among them you will find:\r\n\r\n1. `Drainers` which block draining the stream until there is no more data flowing through and returning any possible errors.\r\n\r\n2. `Collectors` collect all items that reached the end of the pipeline and any possible error.\r\n\r\nSay we have a stream where instances of `Person` are flowing through, then you can collect items off the stream like so:\r\n\r\n```go\r\ntype Person struct {\r\n\tName string\r\n}\r\n\r\ndiego := Person{\"Diego\"}\r\nborges := Person{\"Borges\"}\r\n\r\nitems, err := rivers.FromData(diego, borges).Collect()\r\n// items == []stream.T{Person{\"Diego\"}, Person{\"Borges\"}}\r\n\r\nitem, err := rivers.FromData(diego, borges).CollectFirst()\r\n// item == Person{\"Diego\"}\r\n\r\nitem, err := rivers.FromData(diego, borges).CollectLast()\r\n// item == Person{\"Borges\"}\r\n\r\nvar people []Person\r\nerr := rivers.FromData(diego, borges).CollectAs(&people)\r\n// people == []Person{{\"Diego\"}, {\"Borges\"}}\r\n\r\nvar diego Person\r\nerr := rivers.FromData(diego, borges).CollectFirstAs(&diego)\r\n\r\nvar borges Person\r\nerr := rivers.FromData(diego, borges).CollectLastAs(&diego)\r\n```\r\n\r\n### Transformers ![Dispatching To Streams](https://raw.githubusercontent.com/drborges/rivers/master/docs/transformer.png)\r\n\r\nReads data from a particular stream applying a transformation function to it, optionally forwarding the result to an output channel. Transformers implement the interface `stream.Transformer`\r\n\r\n```go\r\ntype Transformer interface {\r\n\tTransform(in stream.Readable) (out stream.Readable)\r\n}\r\n```\r\n\r\nThere are a variety of transform operations built-in in rivers, to name a few: `Map`, `Filter`, `Each`, `Flatten`, `Drop`, `Take`, etc...\r\n\r\nBasic Stream Transformation Pipeline: `Producer -> Transformer -> Consumer`\r\n\r\n![Basic Stream](https://raw.githubusercontent.com/drborges/rivers/master/docs/stream-transformation.png)\r\n\r\nAiming extensibility, rivers allow you to implement your own version of `stream.Transformer`. The following code implements a `filter` in terms of `stream.Transformer`:\r\n\r\n```go\r\ntype Filter struct {\r\n\tcontext stream.Context\r\n\tfn      stream.PredicateFn\r\n}\r\n\r\nfunc (filter *Filter) Transform(in stream.Readable) stream.Readable {\r\n\treadable, writable := stream.New(in.Capacity())\r\n\r\n\tgo func() {\r\n\t\tdefer filter.context.Recover()\r\n\t\tdefer close(writable)\r\n\r\n\t\tfor {\r\n\t\t\tselect {\r\n\t\t\tcase <-filter.context.Closed():\r\n\t\t\t\treturn\r\n\t\t\tdefault:\r\n\t\t\t\tdata, more := <-in\r\n\t\t\t\tif !more {\r\n\t\t\t\t\treturn\r\n\t\t\t\t}\r\n\t\t\t\tif filter.fn(data) {\r\n\t\t\t\t\twritable <- data\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}()\r\n\r\n\treturn readable\r\n}\r\n```\r\n\r\nNote that the transformer above also takes care of those `3 aspects` mentioned in the `producer` implementation. You could use this transformer like so:\r\n\r\n```go\r\nstream := rivers.FromRange(1, 10)\r\n\r\nevensOnly := func(data stream.T) bool {\r\n\treturn data.(int) % 2 == 0\r\n}\r\n\r\nfilter := &Filter{stream.Context, evensOnly}\r\n\r\nevens, err := stream.Apply(filter).Collect()\r\n```\r\n\r\nIn order to reduce some of the boilerplate, rivers provides a generic implementation of `stream.Transformer` that you can use to implement many use cases: `transformers.Observer`. The filter above can be rewritten as:\r\n\r\n```go\r\nfunc NewFilter(fn stream.PredicateFn) stream.Transformer {\r\n\treturn &Observer{\r\n\t\tOnNext: func(data stream.T, emitter stream.Emitter) error {\r\n\t\t\tif fn(data) {\r\n\t\t\t\temitter.Emit(data)\r\n\t\t\t}\r\n\t\t\treturn nil\r\n\t\t},\r\n\t}\r\n}\r\n\r\nevens, err := rivers.FromRange(1, 10).\r\n\tApply(NewFilter(evensOnly)).\r\n\tCollect()\r\n```\r\n\r\nThe observer `OnNext` function may return `stream.Done` in order to finish its work explicitly stopping the pipeline. This is useful for implementing short-circuit operations such as `find`, `any`, etc...\r\n\r\n### Combiners ![Dispatching To Streams](https://raw.githubusercontent.com/drborges/rivers/master/docs/combiner.png)\r\n\r\nCombining streams is often a useful operation and rivers makes it easy with its pre-baked combiner implementations `FIFO`, `Zip` and `ZipBy`. A combiner implements `stream.Combiner` interface:\r\n\r\n```go\r\ntype Combiner interface {\r\n\tCombine(in ...Readable) (out Readable)\r\n}\r\n```\r\n\r\nIt essentially takes one or more readable streams and gives you back a readable stream which is the result of combining all the inputs.\r\n\r\nCombining Streams Pipeline: `Producers -> Combiner -> Transformer -> Consumer`\r\n\r\n![Combining Streams](https://raw.githubusercontent.com/drborges/rivers/master/docs/stream-combiner.png)\r\n\r\nThe following example combines data from 3 different streams into a single stream:\r\n\r\n```go\r\nfacebookMentions := rivers.From(FacebookPostsProducer(httpClient)).\r\n\tMap(ExtractMentionsTo(\"diego.rborges\"))\r\n\r\ntwitterMentions := rivers.From(TwitterFeedProducer(httpClient)).\r\n\tMap(ExtractMentionsTo(\"dr_borges\")).Stream\r\n\r\ngithubMentions := rivers.From(GithubRiversCommitsProducer(httpClient)).\r\n\tMap(ExtractMentionsTo(\"drborges\")).Stream\r\n\r\nerr := facebookMentions.Merge(twitterMentions, githubMentions).\r\n\tTake(mentionsFrom3DaysAgo).\r\n\tEach(prettyPrint).\r\n\tDrain()\r\n```\r\n\r\n### Dispatchers ![Dispatching To Streams](https://raw.githubusercontent.com/drborges/rivers/master/docs/dispatcher.png)\r\n\r\nDispatchers forward data from a readable stream to one or more writable streams returning a new readable stream from where the  non dispatched data can still be processed.\r\n\r\nDispatchers may conditionally dispatch data based on a `stream.PreficateFn`. The `Partition` operation is an example of conditional dispatcher.\r\n\r\nRivers implement 3 types of dispatchers: `Split`, `SplitN` and `Partition`. Dispatchers implement `stream.Dispatcher` interface:\r\n\r\n```go\r\ntype Dispatcher interface {\r\n\tDispatch(from Readable, to ...Writable) (out Readable)\r\n}\r\n```\r\n\r\nDispatching data to multiple targets in a pipeline would look like: `Producer -> Dispatcher -> Transformers -> Consumers`\r\n\r\n![Dispatching To Streams](https://raw.githubusercontent.com/drborges/rivers/master/docs/stream-dispatcher.png)\r\n\r\nThe following code show a use case for the `Partition` operation mentioned above:\r\n\r\n```go\r\ninactiveUsers, activeUsers := rivers.From(UserSessionsAPI(httpClient)).\r\n\tPartition(inactiveForOver7Days)\r\n```\r\n\r\nThe example above forks the original stream into two others based on the given `predicate`. Data is then dynamically dispatched to either stream based on the predicate result.\r\n\r\nUnder the hood the partition operation makes use of a conditional dispatcher to redirect data to the resulting streams. For more detailed information on how to leverage the built-in dispatchers, take a look at the `dispatchers` package.  \r\n\r\n# Examples\r\n\r\n```go\r\nevensOnly := func(data stream.T) bool { return data.(int) % 2 == 0 }\r\naddOne := func(data stream.T) stream.T { return data.(int) + 1 }\r\n\r\ndata, err := rivers.FromRange(1, 10).\r\n\tFilter(evensOnly).\r\n\tMap(addOne).\r\n\tCollect()\r\n\r\nfmt.Println(\"data:\", data)\r\nfmt.Println(\"err:\", err)\r\n\r\n// Output:\r\n// data: []stream.T{3, 5, 7, 9, 11}\r\n// err: nil\r\n```\r\n\r\n# Built-in Filters and Mappers\r\n\r\nTODO\r\n\r\n# The Cancellation Problem\r\n\r\nImagine you have a program that fires off a great deal of concurrent execution threads, in Go that translates to `goroutines`. Making sure each one of these goroutines come to an end by normally finishing their execution or more importantly canceling themselves upon any fatal failure in the pipeline is an interesting challenge. That is called the `cancellation` problem. \r\n\r\nKeeping track of all potential `goroutines` running in a system might not scale very well specially in scenarios where the number of concurrent goroutines might get to the hundreds of thousands. The concurrency model based on `goroutines` implemented by Go along with its primitives for working with `channels` and panic recovering allow developers to handle problems like this in a very elegant manner.\r\n\r\nRivers solves the `cancellation` problem by applying one of the properties of `closed channels`: [A closed channel is always ready to receive](http://golang.org/ref/spec#Receive_operator).\r\n\r\nEvery rivers operation before producing, consuming or transforming incoming data, first checks whether or not the context is `Done` within a [select channel block](https://gobyexample.com/non-blocking-channel-operations), take the `filter` example previously mentioned:\r\n\r\n```go\r\ngo func() {\r\n\tdefer filter.context.Recover()\r\n\tdefer close(writable)\r\n\r\n\tselect {\r\n\tcase <-filter.context.Done():\r\n\t\treturn\r\n\tdefault:\r\n\t\tdata, more := <-in\r\n\t\tif !more {\r\n\t\t\treturn\r\n\t\t}\r\n\t\tif filter.fn(data) {\r\n\t\t\twritable <- data\r\n\t\t}\r\n\t}\r\n}()\r\n```\r\n\r\nNote that before it filters the incoming data -- default block -- it tries to read from the context `Done` channel. If it is able to read from it then the context Done channel is already closed therefore always read to receive. That causes the select block to peek the context done case, returning and finishing the goroutine right away.\r\n\r\nRivers will close the done channel whenever it recovers from a panic anywhere in the pipeline -- as long as the panicing goroutine defers `context.Recover()`.\r\n\r\nNow imagine every single goroutine fired by rivers applying this pattern, the whole system continuation/cancellation logic can be controlled by simply closing or not a single channel. No need to keep track nor synchronize hundreds of thousands of execution threads, just a single close statement for the rescue. That is how powerful Go concurrency model is <3\r\n\r\n# Going Parallel\r\n\r\nRivers parallelization support is an experimental feature and it is under active development. Currently the following operations suport parallelization: `Each`, `Map`, `Filter` and `OnData`.\r\n\r\nWhen parallelizing an opearation, lets say `Each`, rivers will create a new parallel transformer (`Each` transformer in this case) for each slot in the pipeline capacity (a.k.a the producer's capacity), each of these transformers will be consuming data out of the same input readable stream having their output streams merged into a single pipeline where you can attach new pipeline operations to it.  \r\n\r\nEnabling parallelization is feirly simple, take the following code for instance:\r\n\r\n```go\r\nfacebookMentions := rivers.From(FacebookPostsProducer(httpClient)).\r\n\tMap(ExtractMentionsTo(\"diego.rborges\"))\r\n\r\nerr := facebookMentions.Parallel().\r\n\tDrop(ifAlreadyInDatabase).\r\n\tBatch(500).\r\n\tEach(saveBatchInDatabase).\r\n\tEach(indexBatchInElasticsearch).\r\n\tDrain()\r\n```\r\n\r\nIn the example above the `Drop`, and `Each` operations will be `parallelized`. Assuming the `FacebookPostsProducer` capacity is `1000` items then 1000 parallel transformers will be created for the `Drop` stage and a `1000` more parallel transformers for the `Each` stage.\r\n\r\n# Troubleshooting\r\n\r\nTODO `rivers.DebugEnabled`\r\n\r\n# Future Work / Improvements\r\n\r\n- [ ] Dynamic back-pressure?\r\n- [ ] Monitoring?\r\n\r\n# Contributing\r\n\r\nTODO\r\n\r\n# License\r\n\r\nTODO","google":"UA-67229072-1","note":"Don't delete this file! It's used internally to help with page regeneration."}